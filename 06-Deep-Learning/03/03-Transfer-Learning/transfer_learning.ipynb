{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xp81GSFN7uHY"
   },
   "source": [
    "# Deep Learning week - Day 3 - Exercise 3: Transfer Learning\n",
    "\n",
    "The exercise one made you build your first Convolutional Neural Network.\n",
    "In the second exercise, you implemented first data augmentation techniques to improve the accuracy of your model. \n",
    "But you've probably seen that when building complex convolutional neural networks and using a lot of data, the training started to take some time. \n",
    "\n",
    "\n",
    "# Google Colab\n",
    "\n",
    "\n",
    "To overcome this long training time, you can train your neural networks (especially convolutional neural networks) on Google Colab. Google Colab is nothing more than a way to have online notebooks, with the possibility to use Google GPUs. The idea here is not to use it in production (as there are some limitations) but to use Google Colab to test and prototype new algorithms. This free access to GPU allows you to accelerate the computational time. \n",
    "\n",
    "So the first task is to open the notebook you are reading on Google Colab. To do so: \n",
    "- Open Google Colab [here](https://colab.research.google.com/)\n",
    "- Import a Notebook and select this file, the one you are currently reading.\n",
    "- Once open, you a running a similar Notebook but in Google Colab.\n",
    "- Change the runtime type to GPU - default is CPU.\n",
    "\n",
    "Welcome to Google Colab! Do not forget that you can open any notebook in Google Colab (but it is not always worth it and working on your computer might be enough for many tasks - however, with images, we recommand to do it on Google Colab. Moreover, we will mention the exercises that are preferably done on Colab during the week.).\n",
    "\n",
    "\n",
    "# The exercise\n",
    "\n",
    "\n",
    "This notebook is dedicated to **transfer learning** which corresponds to using an existing model that has been already trained on a particular task, and fine-tuning it for a different task.\n",
    "\n",
    "To that end, we will use the [VGG-16 Neural Network](https://neurohive.io/en/popular-networks/vgg16/), a well-known architecture that has been trained on ImageNet which is a very large database of images of different categories. In a nutshell, this architecture has already learnt kernels which are supposed to be good not only for the task it has been train on but maybe for other tasks. \n",
    "\n",
    "The idea is that first layers are not specialized for the particular task it has been trained on ; only the last one are. Therefore, we will load the existing VGG16 network, remove the last fully connected layers, replace them by new connected layers (whose weights are randomly set), and train these last layers on a specific classification task - here, separate types of flower. The underlying idea is that the first convolutional layers of VGG-16, that has already been trained, corresponds to filters that are able to extract meaning features from images. And you will only learn the last layers for your particular problem.\n",
    "\n",
    "\n",
    "# Data loading & Preprocessing\n",
    "\n",
    "You have two options to load the data on Google Colab.\n",
    "\n",
    "\n",
    "### Option 1: Loading the data directly \n",
    "\n",
    "You can first get the data onto google Colab thanks to:\n",
    "\n",
    "`!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`,\n",
    "\n",
    "and then run \n",
    "\n",
    "`!unzip flowers-dataset.zip`\n",
    "\n",
    "This is a very easy option to load the data into your working directory.\n",
    "\n",
    "\n",
    "### Option2: Adding the data to Google Drive.\n",
    "\n",
    "You can first download the data  from `https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`. Then you have to add them to your Google Drive in a folder called `Deep_learning_data` (for instance) and run the following code in the notebook.: \n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "```\n",
    "\n",
    "The previous code will ask you to go to a given webpage where you copy the link and past it in the Colab form that will appear. Do so to load the data on Google Colab.\n",
    "\n",
    "Why choosing this option over the first one? This can be of interest if you work in a project team, and update the data from time to time. By doing this, you can share the same data folder within a team, and be sure that everyone has the same at any time, even though someone changes it. The drawback is that Google Collab has now access to your Google Folder, which you might not be in favor of, depending on your sensibility.\n",
    "\n",
    "❓ **Question** ❓ Use one of the above method to load your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "Y1c4Xwno7uHm"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riTyl7uf3YDC"
   },
   "source": [
    "❓ **Question** ❓ Depending on the `loading_method` you have used, load the data and store them in the appropriate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "hB0XEe6y7uH7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_flowers_data():\n",
    "    data_path = '/content/drive/My Drive/06-Deep-Learning/02/data/flowers'\n",
    "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (cl, i) in classes.items():\n",
    "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
    "        for img in tqdm(images_path[:300]):\n",
    "            path = os.path.join(data_path, cl, img)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path)\n",
    "                image = image.resize((256, 256))\n",
    "                imgs.append(np.array(image))\n",
    "                labels.append(i)\n",
    "\n",
    "    X = np.array(imgs)\n",
    "    num_classes = len(set(labels))\n",
    "    y = to_categorical(labels, num_classes)\n",
    "\n",
    "    # Finally we shuffle:\n",
    "    p = np.random.permutation(len(X))\n",
    "    X, y = X[p], y[p]\n",
    "\n",
    "    first_split = int(len(imgs) /6.)\n",
    "    second_split = first_split + int(len(imgs) * 0.2)\n",
    "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
    "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's start by some preprocessing\n",
    "\n",
    "❓ **Question** ❓ First check that all your images have the same size. How many colors do these images have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOF7fVp27uI2"
   },
   "source": [
    "❓ **Question** ❓ Plot some images with the `imshow` function of matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "\n",
    "❓ **Question** ❓ As it is already the third exercise, let's make you work a bit on your own to create your own CNN. Here, you are free to write any CNN architecture that is trainable on the flower data. Do not forget to compile it and fit it on the train data (use the `validation_data` with the `X_val` and `y_val` instead of using the `validation_split` argument). Do not forget the early stopping criterion.\n",
    "\n",
    "<details>\n",
    "   <summary>(If you really do not know where to start from, you can build the following model:</summary>\n",
    "    <ul>\n",
    "        <li> A convolution layer with enough filters - choose the kernel size wisely as your data are quite large here</li>\n",
    "        <li> A Maxpooling layer </li>\n",
    "        <li> A second convolution layer </li>\n",
    "        <li> A Maxpooling layer </li>\n",
    "        <li> A third convolution layer </li>\n",
    "        <li> A Maxpooling layer </li>\n",
    "        <li> Flatten your output </li>\n",
    "        <li> Add a dense layer </li>\n",
    "        <li> Add the last layer </li>\n",
    "        \n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "As you already know how to build CNN, you are here welcome to write your own model, compile it and fit it on the train data (use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Play a bit with your model to look at the effect of different architectures, especially by:\n",
    "- changing the number of convolution layers (associated to their maxpooling layer)\n",
    "- changing the number of filters\n",
    "- changing the kernel sizes\n",
    "- changing the padding and strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nr6m5eKs9s54"
   },
   "source": [
    "# Transfer learning: VGG16 model\n",
    "\n",
    "Let's now build our model. \n",
    "\n",
    "❓ **Question** ❓ Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Especially, look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) to load the model where:\n",
    "- the `weights` have been learnt on `imagenet`\n",
    "- the `input_shape` corresponds to the input shape of any of your images - you have to resize them in case they are not of the same size\n",
    "- the `include_top` argument is set to `False` in order not to load the fully-connected layers of the VGG-16 without the last layer which was specifically trained on `imagenet`\n",
    "\n",
    "❗ **Remark** ❗ Do not change the default value of the other arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "def load_model():\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the architecture of the model thanks to the summary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive, right? Two things to notice:\n",
    "- It ends with a convolution layer (namely a maxpooling layer that is the layer that follows a convolution). The flattening of the output and the fully connected layers are not here yet! We will add them\n",
    "- There are more than 14.000.000 parameters, which is a lot. We could fine-tune them, meaning still updates them in the model algorithm, but it will take a lot of time, therefore we will fix them so they are not trainable. And only the last layers (that we will add) will be trained and updates\n",
    "\n",
    "❓ **Question** ❓ Write a function that takes the previous model as input, and does what we just described:\n",
    "- set the first layers to be not-trainable, by applying `model.trainable = False`\n",
    "- add new layers (that by default will be trainable).\n",
    "\n",
    "To do the second part (adding new layers), we will use another way to define the model (instead of `model.sequential()`). Look at the following code and try to apply it in your scenario:\n",
    "\n",
    "```\n",
    "base_model = load_model()\n",
    "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
    "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  dense_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "❗ **Remark** ❗ Let's consider a dense layer with 500 neurons ; on the other hand, the prediction layer should be related to this specific task, so just replace `SOME_NUMBER_2` and `APPROPRIATE_ACTIVATION` with the correct values. \n",
    "\n",
    "\n",
    "❗ **Remark** ❗ Do not forget to Flatten the convoluted outputs first, before the fully connected layers ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def update_model(model):\n",
    "    # Set the first layers to be untrainable\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Add layers to the mdoel\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now look at the layers and parameters of your model. Note that there is a distinction, at the end, between the trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function to compile your model - we advise Adam with `learning_rate=1e-4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write an overall function that :\n",
    "- load the model\n",
    "- update the layers\n",
    "- compiles it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    # YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the data\n",
    "\n",
    "The VGG16 model was trained on images which were preprocessed in a specific way. \n",
    "\n",
    "❓ **Question** ❓ Apply this processing to the images here using the method `preprocess_input` that you can import from `tensorflow.keras.applications.vgg16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B--Gyb-23YDb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wu2H0KZF-EoI"
   },
   "source": [
    "# Run the model\n",
    "\n",
    "❓ **Question** ❓ Now estimate the model, with an early stopping criterion on the validation accuracy - here, the validation data are provided, therefore use `validation_data` instead of `validation_split`.\n",
    "\n",
    "❗ **Remark** ❗ Store the results in a `history` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "z97kx9yUAas5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ec_I9JpiAm-W"
   },
   "source": [
    "❓ **Question** ❓ Plot the accuracy for the test and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3plexlQAtcC"
   },
   "source": [
    "❓ **Question** ❓ Evaluate the model accuracy on the test set. What is the chance level on this classification task (i.e. accuracy of a random classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzU0wCXlB6UI"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzetiM3XA2fu"
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "The next question are a less guided as they directly derive from what you have done in the previous exercise - don't hesitate to come back to what you have done.\n",
    "\n",
    "❓ **Question** ❓ Use some data augmentation techniques for this task - you can store the fitting in a `history_data_aug` variable that you can plot. Do you see an improvement ? Don't forget to evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IvsCub2JBre7",
    "outputId": "2eb5d3fa-5ab4-4968-9c74-5ad1a6a5cef3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF39HIb7BSOy"
   },
   "source": [
    "# Improve the model\n",
    "\n",
    "You can here try to improve the model test accuracy. To do that, here are some options you can consider\n",
    "\n",
    "1) Is my model overfitting ? If yes, you can try more data augmentation. If no, try a more complex model (unlikely the case here)\n",
    "\n",
    "2) Perform precise grid search on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
    "\n",
    "3) Change the base model to more modern one (resnet, efficient nets) available in the keras library\n",
    "\n",
    "4) Curate the data: maintaining a sane data set is one of the keys to success.\n",
    "\n",
    "5) Obtain more data\n",
    "\n",
    "\n",
    "❗ **Remark** ❗ Note also that it is good practice to perform a real cross-validation. You can also try to do that here to be sure of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IyqGWzGBN0Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
