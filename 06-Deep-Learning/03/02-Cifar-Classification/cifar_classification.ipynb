{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning week - Day 3 - Exercise 2\n",
    "\n",
    "Let's move on to bit more complex problem! The goal of this exercise is again to classify images of the CIFAR-10 dataset. This dataset contains images of 10 different classes, being : \n",
    "- airplane \t\t\t\t\t\t\t\t\t\t\n",
    "- automobile \t\t\t\t\t\t\t\t\t\t\n",
    "- bird \t\t\t\t\t\t\t\t\t\t\n",
    "- cat \t\t\t\t\t\t\t\t\t\t\n",
    "- deer \t\t\t\t\t\t\t\t\t\t\n",
    "- dog \t\t\t\t\t\t\t\t\t\t\n",
    "- frog \t\t\t\t\t\t\t\t\t\t\n",
    "- horse \t\t\t\t\t\t\t\t\t\t\n",
    "- ship \t\t\t\t\t\t\t\t\t\t\n",
    "- truck\n",
    "\n",
    "In this notebook, we propose to define a simple baseline CNN to distinguish the 10 categories from the CIFAR-10 dataset.\n",
    "\n",
    "⚠️ **Warning** ⚠️ For now, computations are done on your CPU : bear in mind that a model training will take ~10 minutes in this notebook, so don't waste your trainings !\n",
    "\n",
    "## The data\n",
    "\n",
    "❓ **Question** ❓ Load the data and the associated labels. To load the CIFAR-10 dataset you can use `keras` package ([documentation](https://keras.io/api/datasets/)). What is the shape of the images? How many images do you have per class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(X_train, labels_train), (X_test, labels_test) = cifar10.load_data()\n",
    "\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Normalize your data by dividing all the intensities by the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Display some of the images using the `imshow` function from matplotlib - and print the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Convert the current labels into one-hot encoded labels - stored in `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Neural Network\n",
    "\n",
    "Now, let's define the Convolutional Neural Network - CNN. \n",
    "\n",
    "❓ **Question** ❓ Define a CNN that is composed of:\n",
    "- a Conv2D layer with 16 filters, a kernel size of (4, 4), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a Conv2D layer with 32 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (3, 3)\n",
    "- a Conv2D layer with 64 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (3, 3)\n",
    "- a Flatten layer\n",
    "- a dense function with 75 neurons with the `relu` activation function\n",
    "- a dense function related to your task\n",
    " \n",
    " PS: Do not include the compilation in the function.\n",
    " \n",
    " ⚠️ **Warning** ⚠️ Do not forget to add the input shape of your data to the first layer. And do not forget that it has three colors ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the number of parameters of your model? \n",
    "\n",
    "<details>\n",
    "   <summary>If you don't remember how to check the number of parameters, click here</summary>\n",
    "    `model.summary()`\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function to compile your model. \n",
    "\n",
    "[ Advanced ] It is not mandatory but you can try to use the `adam` optimizer with a learning rate of 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "def compile_model(model):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ After compiling your model, fit it on your training data, with an early stopping (patience to 5 to keep fast computations, and the `restore_best_weights` set to True and `min_delta=1e-2` - you can check what it is in the documentation if interested).\n",
    "\n",
    "Store the output of the fit in an `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Run the following function on the previous history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test data. Are you satisfied with these performances ? What is the chance level on this task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "To easily improve the accuracy of a model without much work, we can generate new data, also called _data augmentation_. It is a very used technique in the case of images as it is possible to apply very easy transformation of your input data without changing the label: mirroring, cropping, intensity changes, etc. This technique is intended to improve the generalization of the model (and thus improving its performance) as you feed your neural network work more images.\n",
    "\n",
    "The first option to generate new data is to take the initial images, do some transformation and concatenate the new images with the previous one. However such procedure can be very memory intensive and at some point, you won't be able to store additionnal data on your computer memory.\n",
    "\n",
    "For this reason, we will augment the data _on the fly_, meaning that we will create new data, use them to fit the model, then delete them. To do that, we will directly use Keras utils which does all that job for us. For that reason, the following functions and objects might seem a bit confusing, but don't be disturb: just look at the function arguments that defines the augmentation techniques that we will use and that you can check in the  [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=(0.8, 1.),\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    rescale=1./255.) \n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now vizualize the input data and what has been generated by the ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "viz_flow = datagen.flow(X_train, shuffle=False, batch_size=1)\n",
    "\n",
    "for i, (raw_image, augmented_image) in enumerate(zip(X_train, viz_flow)):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2))\n",
    "    ax1.imshow(raw_image)\n",
    "    ax2.imshow(augmented_image[0])\n",
    "    plt.show()\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the model - you can look at how the datagen object is used in the fitting procedure.\n",
    "\n",
    "❓ **Question** ❓ Intuitively, do you think the estimation will be faster in terms of number of epochs ? in terms of computation time ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "model_2 = initialize_model()\n",
    "model_2 = compile_model(model_2)\n",
    "\n",
    "# The data generator\n",
    "X_tr = X_train[:30000]\n",
    "y_tr = y_train[:30000]\n",
    "X_val = X_train[30000:]\n",
    "y_val = y_train[30000:]\n",
    "train_flow = datagen.flow(X_tr, y_tr, batch_size=16)\n",
    "\n",
    "# The early stopping criterion\n",
    "es = EarlyStopping(patience=5, verbose=1, restore_best_weights=True, min_delta=1e-2)\n",
    "\n",
    "# The fit\n",
    "history_2 = model_2.fit_generator(train_flow, \n",
    "                                  epochs=100, \n",
    "                                  callbacks=[es], \n",
    "                                  validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, let's plot the previous and current run histories. What do you think of the data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = plot_history(history_2, exp_name='data_augmentation')\n",
    "plot_history(history ,axs=axs, exp_name='baseline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate the model on the test data. Do you see an improvement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flow = datagen.flow(X_test, y_test)\n",
    "model_2.evaluate(test_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remark\n",
    "\n",
    "One thing you have probably noticed in this notebook is that the training is quite long. This is the reason why we stopped training quickly to still have somehow fast experiments. However, in practice, training must be allowed to last longer, with a a stopping criterion that has a lower delta and higher patience!\n",
    "\n",
    "How can we do that?  Actually, when you run the notebook on your compute, you train the neural network on your CPU. However, training neural network on images (in each batch) can be parallelized, and this parallelization procedure can be done on GPU.\n",
    "\n",
    "First, you might face the fact that you don't have a GPU on your computer. Bur more importantly, it can be hard to set up the training on the GPU as it requires special hardware, software and sometimes OS. Therefore, we will look at another way to train our CNN on GPU (for free): Google Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
