{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 5 - Exercise 1\n",
    "\n",
    "Natural Language Processing corresponds to the automated study of language. To deal we texts, we have to convert them into meaningful representations that can be handled by computer. \n",
    "\n",
    "Usually, the first step consists of data cleaning. As you have already done it last week, we won't emphasize this part today. However, if you have finished an exercise, you feel confortable with what you have and you want to get better performances, you are welcomed to improve the data cleaning.\n",
    "\n",
    "✅ **Good Practice** ✅ Never spend to much time on data cleaning! First, build the entire pipeline first to have a baseline evaluation of your task. Otherwise, you don't know whether your fancy data cleaning is improving the entire pipeline or not. \n",
    "\n",
    "After the data cleaning, let's represent each word of our vacabulary as a token (each word corresponds to a integer, each integer corresponds to a word), and then, convert each token to a vector of fixed dimension. In that manner, each word will be represented by a vector that can be fed into a (Recurrent) Neural Network.\n",
    "\n",
    "# The data\n",
    "\n",
    "Sentiment analysis is the task of classifying sentences according to a subjective notion or an affective state. In this notebook, we will perform such an analysis on movie reviews from the imdb database. This database contains thousands of movie reviews and the associated sentiment (positive or negative). We will train differend models to classify the sentences.\n",
    "\n",
    "Let's first load the data. You don't have to understand what is going on in the function, it does not matter here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data():\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    word_to_id = imdb.get_word_index()\n",
    "\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    sentences_train = [[id_to_word[_] for _ in sentence] for sentence in sentences_train]\n",
    "    sentences_test = [[id_to_word[_] for _ in sentence] for sentence in sentences_test]\n",
    "    \n",
    "    return sentences_train, y_train, sentences_test, y_test\n",
    "\n",
    "sentences_train, y_train, sentences_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are loaded, let's look at them\n",
    "\n",
    "❓ **Question** ❓ Plot one of two sentences to understand the sentences_train and sentences_test structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ Look at the y_train and y_test. This is a classification task where, based on the text, you try to predict where the review is negative (=0) or positive (=1). It corresponds to a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding with Word2Vec\n",
    "\n",
    "Now, let's use Word2Vec to embed the words of our sentences. Word2Vec will be able to convert each word to a fixed-size vectorial representation.\n",
    "\n",
    "For instance, we will have:\n",
    "- 'dog' --> [0.1, -0.3, 0.8]\n",
    "- 'cat' --> [-1.1, 2.3, 0.7]\n",
    "- 'apple' --> [3.1, 0.9, -4.7]\n",
    "\n",
    "What you expect is to have representation such as words with close meanings are close in this embedding space, such as on the example on this image:\n",
    "\n",
    "![Embedding](word_embedding.png)\n",
    "\n",
    "Let's run Word2Vec!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can here print all the words that are in the dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the embedded representation of some words.\n",
    "\n",
    "❓ **Question** ❓ Try different words - especially, try non-existing words to see that they don't have any representation (which is perfectly normal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the size of each word? It corresponds to the size of the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ The \"magic\" of Word2Vec, especially the way it creates this embedding, is not discussed in this exercise. There are many many many ressources out there explaning how it works. \n",
    "\n",
    "The only thing to know here is that it actually trains an internal neural network (that you don't see) which, in a nutshell, corresponds to the prediction of a word based on the surroundings words in a sentences. So it choose many splits in the different sentenvecs, applies a window, choose some words as inputs $X$  and a word as output $y$ which it tries to predict, in the embedding space.\n",
    "\n",
    "Once you have a word, you can use the `word2vec.wv.similar_by_vector` function to see what are the closest words to a given one.\n",
    "\n",
    "❓ **Question** ❓ Let's look at the closest word (in the embedding space) to some words. You should see that there is some some of lexical proximity that we were interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write `word2vec.wv.similar_by_vector(word2vec.wv['film'])` as W2V(film) here (for the explanation).\n",
    "\n",
    "As any word is represented as a vector, we can do some arithmetic on the words.\n",
    "For instance, we can do W2V(man) - W2V(woman)\n",
    "\n",
    "❓ **Question** ❓ Do this mathematical operation and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, image for a second that, somehow, the following equality holds true - just for a second\n",
    "\n",
    "    W2V(man) - W2V(woman) = W2V(king) - W2V(queen)\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "    W2V(man) - W2V(woman) + W2V(queen) = W2V(king).\n",
    "\n",
    "❓ **Question** ❓ Let's, just for fun (as it would be foolish of us to think that this equality holds true ...), do the operation W2V(man) - W2V(woman) + W2V(queen) and store it in a `res` variable (which will be a vector of size 100- that you can print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We earlier said that, for any vector, it is possible to see the closest vectors to print the word that they represent.\n",
    "\n",
    "❓ **Question** ❓ Look at the closest vector (thanks to the `word2vec.wv.similar_by_vecto` function) of `res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome, right? \n",
    "\n",
    "You just did some arithmetic operations on words!\n",
    "\n",
    "❓ **Question** ❓ Now, write a function that, given a sentence, returns a matrix that corresponds the embedding of the full sentence, which means that you have to embeed each word one after the other and concatenate the result to output a 2D matrix (be sure that your output is a numpy array)\n",
    "\n",
    "PS: Be sure the asserts are ok! Otherwise, there is a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = ['this', 'movie', 'is', 'probably', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "    \n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (9, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function that, given a list of sentence (each sentence being a list of words/strings), returns a list of embedded sentences (each sentence is a matrix). Apply this function to the train and test sentences\n",
    "\n",
    "Hint: Use the previous function `embed_sentence`\n",
    "\n",
    "❗ **Remark** ❗ You will probably notice that some words you are trying to convert throw errors as they are said not to belong to the dictionnary:\n",
    "- for the test set, this is understandable: some words were not in the train set and thus their embedded representation is unknwon\n",
    "- for the train set, it might be odd but this is actually normal - we will explain it later\n",
    "\n",
    "Nonetheless, change your function `embded_sentence` to skip words that are not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "X_train = embedding(word2vec, sentences_train)\n",
    "X_test = embedding(word2vec, sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To be sure that it worked, the following function should run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embedding(X, ds):\n",
    "    if ds == 'train':\n",
    "        assert(np.shape(X[0]) == (217, 100))\n",
    "    if ds == 'test':\n",
    "        assert(np.shape(X[0]) == (68, 100))\n",
    "    for x in X:\n",
    "        assert(np.shape(x)[1] == 100)\n",
    "    \n",
    "check_embedding(X_train, 'train')\n",
    "check_embedding(X_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Do not forget to pad the data, as yesterday, in order to have tensors that can be divided in batch sizes during the optimization. Store the padedd values in `X_train_pad` and `X_test_pad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The model\n",
    "\n",
    "❓ **Question** ❓ Write a RNN with the following layers:\n",
    "- a masking layer\n",
    "- a LSTM with 13 units and tanh activation function\n",
    "- a Dense with 10 units\n",
    "- a output layer that depends on your task\n",
    "\n",
    "❓ **Question** ❓ Then, compile your model (we advise you to use the rmsprop as the optimizer - at least to begin with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit the model on your padded and embedded data - do not forget the early stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ Here, you can start reading the second exercise if your model takes too long. You can then come back from time to time to see if it is over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Word2Vec\n",
    "\n",
    "Remember that some of the train words used to train the word2vec were not in the word2vec vocabulary? The reason is that word2vec has some arguments that we will dig into.\n",
    "\n",
    "❓ **Question** ❓ The first one is the `size` argument. It corresponds to the size of the embedding space. Learn a new `word2vec_2` model, still trained on the `sentences_train`, but with a smaller or higher `size`.\n",
    "\n",
    "Verify on some words that the corresponding embedding is of your selected size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second important argument is `min_count`. It is a integer that tells you how many occurences a given word should have to be learn in the embedding spave. For instance, let's say that the word \"movie\" appears 1000 times in the corpus and \"simba\" only 2 times. If `min_count=3`, the word \"simba\" will be skipped during the training.\n",
    "\n",
    "The intention is to have only words that are sufficiently present in the corpus to have a robust embedded representation\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_3` model with a `min_count` higher than 5 (which is the default value) and a `word2vec_4` with a `min_count` smaller than 5, and then, compare the size of the vocabulary for all the different word2vec that you have trained (you can choose any `size` you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we say that word2vec has an internal neural network that it optimizes based on some predictions? These predictions actually corresponds to predicting a word based on surrounding words. The surroundings words are in a `window` which corresponds to the number of words taken into account. And you can train the word2vec with different `window` sizes\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_5` model with a `window` different than previously (default is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments you have seen (`size`, `min_count` and `window`) are usually the one that you should start changing to get a better performance for your model.\n",
    "\n",
    "But you can also look at other arguments in the [documentation](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "❓ **Question** ❓ Fit a Neural Network whose input derived from fine-tunned word2vec for which you have choosen different parameters. On top of that, you can improve your accuracy by trying other RNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
