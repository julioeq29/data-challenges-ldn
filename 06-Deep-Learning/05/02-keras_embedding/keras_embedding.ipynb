{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 5 - Exercise 2\n",
    "\n",
    "In the previous exercise, we used the word2vec algorithm to embed the input words, meaning that for each word, we find a vectorial representation that was used to as input of a RNN.\n",
    "\n",
    "However, this vectorial representation was not designed to specifically handled a specific task. For that reason, we will here use an embedding layer in our RNN so that each word is represented in an embedding space as a vector whose values are learnt. For instance, for the word `dog` which is represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, we will learn the weights $(w_k)_k$.\n",
    "\n",
    "# The data\n",
    "\n",
    "The data are the same as in the previous exercises. Each sequence is a sentence, in a form of a list of words, and the output is a negative (0) or positive (1) review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data():\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    word_to_id = imdb.get_word_index()\n",
    "\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    sentences_train = [[id_to_word[_] for _ in sentence] for sentence in sentences_train]\n",
    "    sentences_test = [[id_to_word[_] for _ in sentence] for sentence in sentences_test]\n",
    "    \n",
    "    return sentences_train, y_train, sentences_test, y_test\n",
    "\n",
    "sentences_train, y_train, sentences_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "❓ **Question** ❓ Let's start with some data cleaning. You are welcomed to do whatever you want here. The idea is that everything should hold in the following function (that can call other functions that you can write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Create a dictionary `word_to_id` such that each word ot `sentences_train` is in this dictionnary. The value of each key (=word) should be unique, so that each word is represented by a unique integer - and vice-versa! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ The number of keys should be equal to the number of different words in the train sentences. Print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Create a dictionary `id_to_word` that is the \"reciprocal\" form of `word_to_id`. The keys are the integers and the corresponding value is the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Currently, your sentences are like ['this', 'is', 'a', 'very, 'very', 'good', 'movie\"]. Thanks to the `word_to_id` dictonary, replace them their tokenized representation : [3, 89, 2, 13, 13, 56, 32] (example, not true). Store them in `sentences_token_train` and `sentences_token_test`\n",
    "\n",
    "❗ **Remark** ❗ It is better writting a function to do that. And don't forget that there is no reason all the words in the test sentences to be in this dictionary - in that case, you can skip the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "❓ **Question** ❓ Pad your sentences (results to store in `X_train` and `X_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "❓ **Question** ❓ Write a model that has:\n",
    "- a masking layer (not to take into account padded values)\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (should be an argument of the function), and whose `output_dim` is the embedding size that you can choose\n",
    "- a LSTM or RNN layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(vocab_size):\n",
    "    # YOUR CODE HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Initialize the model and look at the number of parameters. What do you think about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model on training data and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably don't want to spend the rest of your week-end running this model. One of the reason is that there are too many parameters, due to the fact that you embed a lot of different words.\n",
    "\n",
    "❓ **Question** ❓ To reduce the number of words (if you haven't done it already), go back to the data cleaning and, in the train sentences, keep only words that occurs more than 15 times (15 is an example, you can take more), and check how your vocabulary size is reduced. Then, you can run your entire pipeline and see how many parameters there are in the model\n",
    "\n",
    "❗ **Remark** ❗ Be carefull, some reviews have all their words below some given number of occurences. When you skip the given words, you may end up with sentences without any word in it. You have to handle that, as you cannot have an empty $X$. In the case it happens, advice is given to replace by a vector of zeros for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model - more advanced.\n",
    "\n",
    "Let's train some fancy network here. \n",
    "Each of your word is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). So, all your sentences are actually represented as matrices once embeded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate of the feature space.\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolution on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2 dimensional as the filters can move up and down, and left and right. In the case of our sentences, we want the corresponding kernel to move _only_ in the word by word direction (otherwise, moving coordinate of the embedding space by coordinate doesn't make much sense).\n",
    "\n",
    "So let's create a model that use convolution\n",
    "\n",
    "### First, the data\n",
    "\n",
    "❓ **Question** ❓ In the case of convolutions, the input images are of the same size, which is not the case here.  Therefore, we will use the `pad_sequences` function with an additional argument, the `maxlen`. It actually pads any input to a given length, this `maxlen`. Shorter sequences are padded with 0, while longer are cropper to this `maxlen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 1D Convolution.\n",
    "\n",
    "❓ **Question** ❓ Define a model that has :\n",
    "- an Masking layer\n",
    "- an Embedding layer: input_dim is the vocab_size, output_dim is the embedding space dimension and input_length is the max length of your observation (that you just defined in the previous question)\n",
    "- a conv1D layer \n",
    "- a Flatten layer\n",
    "- a dense layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly\n",
    "\n",
    "❗ **Remark** ❗ The size of the Conv1D kernel corresponds exactly to the number of words each kernel is taking into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn_model(vocab_size):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters and compare it to the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model with a stopping criterion, and evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
