{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Deploy to Production with Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the previous exercises, you worked on building a robust machine learning pipeline to build a model that predicts NYC taxi fares.\n",
    "However, you realized the training set is pretty big and it takes a lot of time to train models locally. And because you want to iterate quickly, it does not make sense to waste hours of your time waiting for a training to complete.\n",
    "\n",
    "**But there is a solution!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gstatic.com/devrel-devsite/prod/vcd1bbe5dda31d2b800805cc4c730b0229f847f2d108be33386b6e78644e79178/cloud/images/cloud-logo.svg\" width=\"200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this series of exercises, you will learn how to deploy your code to [Google Cloud Platform](https://cloud.google.com/) aka **GCP** and in particular how to use **[AI Platform](https://cloud.google.com/ai-platform/)** in order to leverage the power of distributed computing and speedup your ML experimentation.\n",
    "\n",
    "Beyond training models, you will see how you can make you models available to the world, manage different versions and serve predictions at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cloud.google.com/ai-platform/images/ml-workflow.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [GCP Setup](#part1)\n",
    "2. [Deploy and train a simple model](#part2)\n",
    "2. [Make predictions from simple model](#part3)\n",
    "4. [Deploy and train a model with a pipeline and dependencies](#part4)\n",
    "5. [Make predictions from pipeline model](#part5) \n",
    "6. [Train at scale](#part6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GCP Setup <a id=\"part1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Project\n",
    "\n",
    "- Go to [Google Cloud](https://console.cloud.google.com/) and create an account if you do not already have one.\n",
    "- In the Cloud Console, on the project selector page, select or create a Cloud project. You can name it `WagonBootcamp` for example\n",
    "- Make sure that billing is enabled for your Google Cloud project. Don't worry, as a first time user, you have a **$300 credit** to use for Google Cloud resources, which will be more than enough for this project.\n",
    "- [Enable the AI Platform Training & Prediction and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component&_ga=2.269215094.662509797.1580849510-2071889129.1567861089&_gac=1.154971594.1580849512.CjwKCAiAyeTxBRBvEiwAuM8dnbZ6uMwizbZW44J2mBCX6ncEjwjwpgF8S8QsvhYAXLkJ8awDnIRTNRoCJ_0QAvD_BwE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Cloud sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install new cli called gcloud  \n",
    "\n",
    "- On Mac :\n",
    "```bash\n",
    "brew cask install google-cloud-sdk\n",
    "```\n",
    "- On Windows follow link bellow\n",
    "[Install and initialize the Cloud SDK](https://cloud.google.com/sdk/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a service account key\n",
    "- Go to [Service Account key page](https://console.cloud.google.com/apis/credentials/serviceaccountkey) \n",
    "- Create a new Service Account key :\n",
    "  - Give whatever name you want to that account\n",
    "  - Set Role as `project > owner`\n",
    "  \n",
    "- Download json, and store it under `~/[FILE_NAME].json`\n",
    "- Then add `export GOOGLE_APPLICATION_CREDENTIALS=\"~/[FILE_NAME].json\"`:  \n",
    "   - to your `~/.aliases` for mac & linux\n",
    "   - to your `.bash_profile` for windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "`AccessDeniedException: 403 The project to be billed is associated with an absent billing account.`\n",
    "\n",
    "- Make sure that billing is enabled for your Google Cloud Platform project.\n",
    "https://cloud.google.com/billing/docs/how-to/modify-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bucket\n",
    "You will need a Google Cloud bucket to store data, code and trained models.\n",
    "For this you have two ways\n",
    "- From UI\n",
    "    - Go to [Storage](https://console.cloud.google.com/storage) and create a bucket from there\n",
    "- Programmatically (**recommended**)\n",
    "    - Use `gsutil mb` command line. See doc [here](https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-gsutil)\n",
    "    \n",
    "##### Exercise\n",
    "Let's use the command line tool in order to automate the workflow as much as possible  \n",
    "Also to make things easier, you are going to create a `Makefile` to run the commands.\n",
    "\n",
    "- Create a `Makefile` \n",
    "```bash \n",
    "touch Makefile\n",
    "```\n",
    "\n",
    "- Implement command that creates the bucket you need in your project.  \n",
    "**IMPORTANT**: Bucket Names must be **globally unique**, please respect convention `wagon-ml-[YOUR_LAST_NAME]-XX`\n",
    "\n",
    "\n",
    "- Run this Makefile (you have the example bellow)\n",
    "- Check on [Storage](https://console.cloud.google.com/storage) that your bucket is correctly created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Makefile\n",
    "\n",
    "```bash\n",
    "PROJECT_ID=XXXX\n",
    "BUCKET_NAME=wagon-ml-[YOUR_LAST_NAME]-XX\n",
    "REGION=europe-west1\n",
    "\n",
    "set_project:\n",
    "    -@gcloud config set project ${PROJECT_ID}\n",
    "\n",
    "create_bucket:\n",
    "\t-@gsutil mb -l ${REGION} -p ${PROJECT_ID} gs://${BUCKET_NAME}\n",
    "\n",
    "all: set_project create_bucket\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to your bucket\n",
    "Now let's upload the data to your bucket.\n",
    "- Make you sure have the data on your local disk\n",
    "- Add `upload_data` command in the Makefile that uploads `train.csv` and `test.csv` to `/data` folder in your bucket  \n",
    "**HINT** use `gsutil cp` command => get help with [GCP documentation](https://cloud.google.com/storage/docs/uploading-objects)\n",
    "- Run command:\n",
    "```bash\n",
    "make upload_data\n",
    "```\n",
    "- Check on [Storage](https://console.cloud.google.com/storage) that your file was correctly uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "`wagon-XX@wagon-bootcamp-XXXXX.iam.gserviceaccount.com does not have storage.objects.list access to wagon-ml-[YOUR_NAME]-XX`\n",
    "\n",
    "- Make sure that you secret key is correctly appended to your .aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy and train a simple model <a id=\"part2\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get familiar with Google AI Platform, we will first deploy and train a simple model for the TaxiFare Challenge.\n",
    "\n",
    "This model will be a linear model **`fare_amount ~ C * distance`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we need to create a python module. For this, create the following structure:\n",
    "- `TaxiFareModelSimple`\n",
    "  - `__init__.py`\n",
    "  - `trainer.py`\n",
    "- `Makefile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainer.py\n",
    "We are going to have all code in  `trainer.py` file.\n",
    "- Write `get_data` method that loads data from your bucket (never forget that **simple is smart** and **pandas is great**)\n",
    "- Write `save_model` method that upload the joblib file to `/models` folder. Give a unique name to your file, by appending a timestamp for example  \n",
    "Good practice to check on [GCP Documentation](https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbizot/venv3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# trainer.py\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"method to get the training data (or a portion of it) from google cloud bucket\"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_distance(df):\n",
    "    \"\"\"method to compute distance of df\"\"\"\n",
    "    pass\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"method that pre-process the data\"\"\"\n",
    "    df[\"distance\"] = compute_distance(df)\n",
    "    X_train = df[[\"distance\"]]\n",
    "    y_train = df[\"fare_amount\"]\n",
    "    return X_train, y_train\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"method that trains the model\"\"\"\n",
    "    clf = linear_model.Lasso(alpha=0.1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    \n",
    "def save_model(clf):\n",
    "    \"\"\"Save the model into a .joblib and upload it on Google Storage /models folder\n",
    "    HINTS : use sklearn.joblib (or jbolib) libraries and google-cloud-storage\"\"\"\n",
    "    pass\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     df = get_data()\n",
    "#     X_train, y_train = preprocess(df)\n",
    "#     clf = train_model(X_train, y_train)\n",
    "#     save_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Makefile\n",
    "\n",
    "Then create the `Makefile` that will have a command to submit `trainer.py` to AI Plaform for training.  \n",
    "Before anything run train.py locally and inspect on GCP Storage if your model is correctly uploaded\n",
    "- Write command for `submit_training`\n",
    "- Tip: https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "- Do not spend more than 20 minutes on that task, feel free to call TA for help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "PACKAGE_NAME=XXXX\n",
    "FILENAME=XXX\n",
    "BUCKET_NAME=XXX\n",
    "PROJECT_ID=XXX\n",
    "JOB_NAME=XXXX\n",
    "REGION=XXX\n",
    "PYTHON_VERSION=XXX\n",
    "RUNTIME_VERSION=XXX\n",
    "FRAMEWORK=XXX\n",
    "MODEL_NAME=XXX\n",
    "\n",
    "set_project:\n",
    "    -@gcloud config set project ${PROJECT_ID}\n",
    "\n",
    "submit_training:\n",
    "    ## write command line to submit training\n",
    "\n",
    "all: set_project submit_training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then:\n",
    "- Run the Makefile\n",
    "- Go to [Jobs page](https://console.cloud.google.com/ai-platform/jobs) and check that your training task has been submitted and completes succesfully\n",
    "- Once the training task is complete, go to the models folder (`/models`) and verify the model file has correctly been uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model and a version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use your model, you need to create a model resource on AI Platform and link a version to the model file.\n",
    "- Go https://console.cloud.google.com/ai-platform/models and create a model\n",
    "- Then on the model page, create a new version and specify the Google Cloud Storage path where you uploaded the model file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions from simple model <a id=\"part3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained your model on the cloud, let's use it to make predictions on the test set!\n",
    "\n",
    "We are going to write a `predict.py` script that will make predictions on a sample test set. \n",
    "\n",
    "#### Exercise\n",
    "- Fill in the missing methods of `predict.py`\n",
    "- Run `predict.py` to generate predicitions on test set\n",
    "- Submit these predictions to Kaggle\n",
    "\n",
    "\n",
    "Documentation: https://cloud.google.com/ai-platform/training/docs/python-client-library?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict.py\n",
    "\n",
    "import googleapiclient.discovery\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "BUCKET_NAME = \"XXXX\"\n",
    "VERSION = \"XXXX\"\n",
    "PROJECT_ID = 'XXX'\n",
    "MODEL = 'XXXX'\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction. \"\"\"\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "    return response['predictions']\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\" load test data from google cloud bucket\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    preprocess method. This should be identical to the preprocess method\n",
    "    that was used from training.\n",
    "    \"\"\"\n",
    "    X_test, y_test = None, None\n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "def convert_to_json_instances(X_test):\n",
    "    return X_test.values.tolist()\n",
    "\n",
    "\n",
    "# df = get_test_data().head(100) # only predict for first 100 rows\n",
    "# X_test, y_test = preprocess(df)\n",
    "# instances = convert_to_json_instances(X_test)\n",
    "# results = predict_json(project=PROJECT_ID, model=MODEL, instances=instances, version=VERSION)\n",
    "# df[\"fare_amount\"] = results\n",
    "# df[[\"key\", \"fare_amount\"]].to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Deploy and train a model with a pipeline and dependencies<a id=\"part4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you succefully trained a model on GCP and then used it to make predictions, let's see how you can do the same thing with a more complex model. \n",
    "One limitation we have seen in the previous case, is that you need to duplicate the pre-processing logic during training and scoring.\n",
    "\n",
    "**With pipelines you can save directly in the trained model the pre-processing part so that you do not have do duplicate code when making predictions**\n",
    "\n",
    "In this exercise, you are going to use the code we have from day 2-3 (with Sklearn pipeline).\n",
    "\n",
    "The main difficulty here is that you will have to upload the code dependencies that are needed to run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Create a folder structure like this one\n",
    "- TaxiFareModel\n",
    "  - `__init__.py`\n",
    "  - `trainer.py`\n",
    "  - `encoders.py`\n",
    "  - `utils.py`\n",
    "- re-use code we have from day 2-3 to train the model using a `Sklearn pipeline` and our custom encoders (`DistanceTransformer` and `TimeFeaturesEncoder`).\n",
    "- `data.py` needs to be updated to load data from google cloud bucket (instead of local file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trainer.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from TaxiFareModel.data import Data\n",
    "from TaxiFareModel.encoders import TimeFeaturesEncoder, DistanceTransformer\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"trainer class that trains the model\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def get_pipeline(self):\n",
    "        \"\"\"builds pipeline\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"trains the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"saves model to google cloud bucket\"\"\"\n",
    "        pass\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    t = Trainer()\n",
    "    t.train()\n",
    "    t.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makefile\n",
    "Now we need a Makefile to submit the training code to GCP.\n",
    "\n",
    "#### Exercise:\n",
    "- Write this new Makefile, making sure you also upload the code dependencies. Read this [doc](https://cloud.google.com/ai-platform/training/docs/packaging-trainer) to help you out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make predictions from pipeline model <a id=\"part5\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once your new model has been submitted and trained, you need to:\n",
    "1. Create a new version for this model with a custom predictions routine\n",
    "2. Call the model to make predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Custom predictions routine\n",
    "Because your input data is Pandas DataFrame, you need to create a customer `Predictor` class to tell the predictions Google Cloud service how to handle input data.\n",
    "\n",
    "Read this doc for more details [Custom Prediction Routines](https://cloud.google.com/ai-platform/prediction/docs/custom-prediction-routines?hl=en)\n",
    "\n",
    "- create a `Predictor` class with:\n",
    "  - `predict(self, instances, **kwargs)` method \n",
    "  - `from_path(cls, model_dir)` classmethod\n",
    "- Then write a new command in your `Makefile` called `create_model_version` that will create a new model version with your custom `Predictor` class. Look for [https://cloud.google.com/sdk/gcloud/reference/beta/ai-platform/versions/create](https://cloud.google.com/sdk/gcloud/reference/beta/ai-platform/versions/create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictor.py\n",
    "\n",
    "class Predictor(object):\n",
    "\n",
    "    def predict(self, instances, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your new version with a custom prediction routine, you can call it to make predictions on the test set.\n",
    "\n",
    "- Write a script `predict.py` that will call this new version for the entire test set.\n",
    "- Submit these predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict.py\n",
    "\n",
    "BUCKET_NAME = \"XXXX\"\n",
    "VERSION = \"XXX\"\n",
    "MODEL = \"XXX\"\n",
    "PROJECT = \"XXX\"\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to the deployed model for prediction. \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\" load test data \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def convert_to_json_instances(X_test):\n",
    "    pass\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     df = get_data()\n",
    "#     instances = convert_to_json_instances(df)\n",
    "#     results = predict_json(project=PROJECT, model=MODEL, instances=instances, version=VERSION)\n",
    "#     df[\"fare_amount\"] = results\n",
    "#     df[[\"key\", \"fare_amount\"]].to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train at scale <a id=\"part6\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to go back to our original goal which is building the best model. \n",
    "- Try to change machine types and scale tiers in order to train your model with the entire dataset faster [https://cloud.google.com/ai-platform/training/docs/machine-types](https://cloud.google.com/ai-platform/training/docs/machine-types)\n",
    "- Perform extenstive hyperparameters tuning to fine tune your model\n",
    "- Submit your predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
