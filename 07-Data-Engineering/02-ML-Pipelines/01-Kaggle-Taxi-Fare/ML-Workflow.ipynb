{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this exercise is to use the tools and methods you learnt during the previous weeks, in order to solve a **real challenge**.\n",
    "\n",
    "The problem to solve is a **Kaggle Competition**: [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction). The goal is to predict the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a machine learning model requires a few different steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. [Get the data](#part1)\n",
    "2. [Explore the data](#part2)\n",
    "3. [Data cleaning](#part3)\n",
    "4. [Evaluation metric](#part4)\n",
    "5. [Model baseline](#part5)\n",
    "6. [Build your first model](#part6)\n",
    "7. [Model evaluation](#part7)\n",
    "8. [Kaggle submission](#part8)\n",
    "9. [Model iteration](#part9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on [Kaggle](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data)\n",
    "\n",
    "First of all:\n",
    "- Follow the instructions to download the training and test sets\n",
    "- Put the datasets in a separate folder on your local disk, that you can name \"data\" for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use Pandas to read and explore the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is relatively big (~5GB). \n",
    "So let's only open a portion of it.  \n",
    "ðŸ‘‰ Go to [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) to see how to open a portion of csv file and store it into a dataframe. (ex: just read 1 million rows maximum)  \n",
    "ðŸ’¡ NB: here we will read portion of a file **directly from an url**, texactly the same can be done with local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 's3://wagon-public-datasets/taxi-fare-train.csv'\n",
    "df = pd.read_csv(url, nrows=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the first rows to understand the different fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to solve the prediction problem, we need to get a better understanding of the data. \n",
    "For that, libraries like Pandas and Seaborn are your best friends. \n",
    "Firt of all, make you sure you have [Seaborn](https://seaborn.pydata.org/) installed and import it into your notebook. Note that this can be also useful to import `matplotlib.pyplot` to customize a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.figure(figsize=(12,5))\n",
    "palette = sns.color_palette('Paired', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are multiple things we want to do in terms of data exploration.\n",
    "\n",
    "- You first want to look at the distribution of the variable you are going to predict: \"fare_amount\"\n",
    "- Then you want to vizualize other variable distributions\n",
    "- And finally it is often very helpful to compute and vizualise correlation between the target the variable and other variables.\n",
    "- Also, look for any missing values, or other irregularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the target variable\n",
    "- Compute simple statistics of the target variable (min, max, mean, std, ...)\n",
    "- Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare_amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_dist(series=df[\"fare_amount\"], title=\"Fare Distribution\"):\n",
    "    sns.distplot(series)\n",
    "    sns.despine()\n",
    "    plt.title(title);\n",
    "    plt.show()\n",
    "plot_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop absurd values \n",
    "df = df[df.fare_amount.between(0, 6000)]\n",
    "plot_dist(df.fare_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualise binned fare_amount variable\n",
    "df['fare-bin'] = pd.cut(df['fare_amount'], bins = list(range(0, 50, 5))).astype(str)\n",
    "\n",
    "# Uppermost bin\n",
    "df.loc[df['fare-bin'] == 'nan', 'fare-bin'] = '[45+]'\n",
    "\n",
    "# Adjust bin so the sorting is correct\n",
    "df.loc[df['fare-bin'] == '(5, 10]', 'fare-bin'] = '(05, 10]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"fare-bin\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore other variables\n",
    "\n",
    "- passenger_count (statistics + distribution)\n",
    "- pickup_datetime (you need to build time features out of pickup datetime)\n",
    "- Geospatial features (pickup_longitude, pickup_latitude,dropoff_longitude,dropoff_latitude)\n",
    "- Find other variables you can compute from existing data that might explain the target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"passenger_count\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Passenger Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickup Datetime \n",
    "- Extract time features from pickup_datetime (hour, day of week, month, year)\n",
    "- Create a method `def extract_time_features(_df)` that you will be able to re-use later\n",
    "- Be careful of timezone\n",
    "- Explore the newly created features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df):\n",
    "    timezone_name = 'America/New_York'\n",
    "    time_column = \"pickup_datetime\"\n",
    "    df.index = pd.to_datetime(df[time_column])\n",
    "    df.index = df.index.tz_convert(timezone_name)\n",
    "    df[\"dow\"] = df.index.weekday\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"year\"] = df.index.year\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour of day\n",
    "sns.catplot(x=\"hour\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Hour of Day');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week\n",
    "sns.catplot(x=\"dow\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Day of Week');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geospatial Data\n",
    "- Extract time features from pickup_datetime (hour, day of week, month, year)\n",
    "- Create a method `def extract_time_features(_df)` that you will be able to re-use later\n",
    "- Be careful of timezone\n",
    "- Explore the newly created features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find boudaries from test and remove them from training set\n",
    "for col in [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]:\n",
    "    MIN = df_test[col].min()\n",
    "    MAX = df_test[col].max()\n",
    "    print(col, MIN, MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"pickup_latitude\"].between(left = 40, right = 42 )]\n",
    "df = df[df[\"pickup_longitude\"].between(left = -74.3, right = -72.9 )]\n",
    "df = df[df[\"dropoff_latitude\"].between(left = 40, right = 42 )]\n",
    "df = df[df[\"dropoff_longitude\"].between(left = -74, right = -72.9 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sur your install folium first\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "center_location = [40.758896, -73.985130]\n",
    "m = folium.Map(location=center_location, control_scale=True, zoom_start=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "df[\"count\"] =1\n",
    "heatmap_data = df.head(10000)[['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\n",
    "gradient = {0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}\n",
    "HeatMap(data=heatmap_data, radius=5, gradient=gradient, max_zoom=13).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data_by_hour = []\n",
    "__df__ = df.head(10000)\n",
    "for hour in df.hour.sort_values().unique():\n",
    "    _df = __df__[__df__.hour == hour][['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\n",
    "    heatmap_data_by_hour.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2 = folium.Map(location=center_location, control_scale=True, zoom_start=11)\n",
    "HeatMapWithTime(heatmap_data_by_hour, radius=5, \n",
    "                gradient=gradient, \n",
    "                min_opacity=0.5, max_opacity=0.8, \n",
    "                use_local_extrema=False).add_to(m2)\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance\n",
    "- Compute distance between pickup and dropoff location (tip: https://en.wikipedia.org/wiki/Haversine_formula)\n",
    "- Write a method `def haversine_distance(df, **kwargs)` that you will be able to reuse later\n",
    "- Compute a few statistics for distance and plot distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def haversine_distance(df,\n",
    "                         start_lat=\"start_lat\",\n",
    "                         start_lon=\"start_lon\",\n",
    "                         end_lat=\"end_lat\",\n",
    "                         end_lon=\"end_lon\"):\n",
    "    \"\"\"\n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees).\n",
    "        Vectorized version of the haversine distance for pandas df\n",
    "        Computes distance in kms\n",
    "    \"\"\"\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n",
    "    dlon = lon_2_rad - lon_1_rad\n",
    "    dlat = lat_2_rad - lat_1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_distance = 6371 * c\n",
    "    return haversine_distance\n",
    "\n",
    "df[\"distance\"] = haversine_distance(df, \n",
    "                                   start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                   end_lat=\"dropoff_latitude\", end_lon=\"pickup_longitude\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "g = sns.distplot(df[df.distance < 50].distance)\n",
    "sns.despine()\n",
    "plt.title(\"Distance distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore how target variable correlate with other variables\n",
    "- As a first step, you can vizualize the target variable vs another variable. For categorical variables, it is often useful to compute the average target variable for each category (Seaborn as plots that do it for you!). For continuous variables (like distance, you can use scatter plots, or regression plots, or bucket the distance into different bins.\n",
    "- But there many different ways to visualize correlation between features, so be creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.catplot(x=\"passenger_count\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"hour\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dow\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"distance\", y=\"fare_amount\", data=df[df.distance < 80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"distance\", y=\"fare_amount\", hue=\"passenger_count\", data=df[df.distance < 80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably identified in the previous section during your data exploration, there are some values that do not seem valid.\n",
    "In this section, you will take a few steps to clean the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all trips that look incorrect. We recommand you writing a method `clean_data(df)` that you will be able to re-use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trips with negative fares:\", len(df[df.fare_amount <= 0]))\n",
    "print(\"trips with too high distance:\", len(df[df.distance >= 100]))\n",
    "print(\"trips with too many passengers:\", len(df[df.passenger_count > 8]))\n",
    "print(\"trips with zero passenger:\", len(df[df.passenger_count == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, test=False):\n",
    "    df = df.dropna(how='any', axis='rows')\n",
    "    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n",
    "    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "    df = df[df.fare_amount.between(0, 4000)]\n",
    "    df = df[df.passenger_count < 8]\n",
    "    df = df[df.passenger_count >= 0]\n",
    "    df = df[df[\"pickup_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"pickup_longitude\"].between(-74.3, -72.9 )]\n",
    "    df = df[df[\"dropoff_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"dropoff_longitude\"].between(-74, -72.9)]\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_data(df)\n",
    "\"% data removed\", (1 - len(df_cleaned) / len(df)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Evaluation metric <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The evaluation metric for this competition is the root mean-squared error or RMSE. RMSE measures the difference between the predictions of a model, and the corresponding ground truth. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better.\n",
    "\n",
    "More details here https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a method `def compute_rmse(y_pred, y_true)` that computes the RMSE given `y_pred` and `y_true` which are two numpy arrays corresponding to model predictions and ground truth values.\n",
    "\n",
    "This method will be useful to evaluate performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(y_pred, y_true):\n",
    "    return np.sqrt(((y_pred - y_true)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Model baseline <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before building your model, it is often useful to get a performance benchmark. For this, you will use a baseline model that is a very dumb model and compute the evualation metric on that model.\n",
    "Then, you will be able to see how much better your model is compared to the baseline. It is very common to see ML teams comming up with very sophisticated approaches without knowing by how much their model beats the very simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Generate predictions based on a simple heuristic\n",
    "- Evaluate RMSE for these predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = df_cleaned.fare_amount.mean()\n",
    "df[\"y_pred\"] = df_cleaned.fare_amount.mean()\n",
    "compute_rmse(df.y_pred, df.fare_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Build your first model <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it is time to build your model!\n",
    "\n",
    "To start we are going to use a linear model only. We will try more sophisticated models later during day 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the different steps you have to follow:\n",
    "\n",
    "1. Split the data into two different sets (training and validation). You will be measuring the performance of your model on the validation set.\n",
    "2. Make sure you apply the data cleaning on your training set\n",
    "3. Think about the different features you want to add in your model\n",
    "4. For each of these features, make sure you apply the correct transformation so that the model can correctly learn from them (this is true for categorical variables like `hour of day` or `day of week`)\n",
    "5. Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# training/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Apply data cleaning on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train = clean_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### List features (continuous vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# features\n",
    "target = \"fare_amount\"\n",
    "features = [\"distance\"]\n",
    "categorical_features = [\"hour\", \"dow\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Features transformation\n",
    "- Write a method `def transform_features(df, **kwargs)` because you will have to make sure you apply the same transformation on the validation (or test set) before making predictions\n",
    "- For categorical features transformation, you can use `pandas.get_dummies` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transform_features(_df, dummy_features=None):\n",
    "    encode = True if dummy_features is None else False\n",
    "    dummy_features = dummy_features if dummy_features is not None else []\n",
    "    for c in categorical_features:\n",
    "        dummies = pd.get_dummies(_df[c], prefix=c)\n",
    "        _df = pd.concat([_df, dummies], axis=1)\n",
    "        if encode:\n",
    "            dummy_features = dummy_features + (list(dummies.columns.values))\n",
    "    for dummy_feature in [f for f in dummy_features if f not in _df.columns]:\n",
    "        _df[dummy_feature] = 0 \n",
    "    _df = _df[dummy_features + features]\n",
    "    return _df, dummy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "from sklearn.linear_model import LassoCV\n",
    "clf = LassoCV(cv=5, n_alphas=5)\n",
    "X_train, dummy_features = transform_features(df_train)\n",
    "y_train = df_train.fare_amount\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. Model evaluation <a id='part7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now to evaluate your model, you need to use your previously trained model to make predictions on the validation set. \n",
    "\n",
    "For this, follow these steps:\n",
    "1. Apply the same transformations on the validation set\n",
    "2. Make predictions\n",
    "3. Evaluate predictions using `compute_rmse` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_val, _ = transform_features(df_val, dummy_features=dummy_features)\n",
    "df_val[\"y_pred\"] = clf.predict(X_val)\n",
    "compute_rmse(df_val.y_pred, df_val.fare_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8. Kaggle submission <a id='part8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that you have a model, you can now make predictions on Kaggle test set and be evaluated by Kaggle directly.\n",
    "\n",
    "- Download test data from Kaggle\n",
    "- Follow [instructions](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation) to make sure your predictions are in the right format\n",
    "- Re-train your model using all the data (do not split between train/validation)\n",
    "- Apply all features engineering and transformations methods on the test set\n",
    "- Use the model to make predictions on the test set\n",
    "- Submit your predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test[\"distance\"] = haversine_distance(df_test, \n",
    "                                   start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                   end_lat=\"dropoff_latitude\", end_lon=\"pickup_longitude\"\n",
    "                                  )\n",
    "df_test = extract_time_features(df_test)\n",
    "X_test, _ = transform_features(df_test, dummy_features=dummy_features) \n",
    "df_test[\"y_pred\"] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True)[[\"key\", \"y_pred\"]].rename(columns={\"y_pred\": \"fare_amount\"}).to_csv(\"lasso_v0_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Push further Feature Engineering <a id='part9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve your model by trying different things (But dont' worry, some of these things will be covered in the next days).\n",
    "- Use more data to train\n",
    "- Build and add more features \n",
    "- Try different estimators\n",
    "- Adjust your data cleaning to remove more or less data\n",
    "- Tune the hyperparameters of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On following section we will focus on advanced Feature Engineering (keep in mind that relevant feateng is often key to significant increase in model performances):\n",
    "\n",
    "ðŸ‘‰ **Manhattan distance** better suited to our problem  \n",
    "ðŸ‘‰ **Distance to NYC center** to highlight interesting pattern ...  \n",
    "ðŸ‘‰ **Direction**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Another Distance ?\n",
    "- Think about the distance you used, try and find a more adapted distance to our problem (Ask TA for insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Minkowski distance](https://wikimedia.org/api/rest_v1/media/math/render/svg/4ed8b780e0d3224880760b1745c444481590ee86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minkowski Distance is actually the generic distance to compute differnet distance\n",
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidian distance = minkowski_distance(x1, x2, y1, y2, p) where p=2\n",
    "# manhattan distance = minkowski_distance(x1, x2, y1, y2, p) where p=1\n",
    "df['manhattan_dist'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n",
    "                                       df['pickup_latitude'], df['dropoff_latitude'], 1)\n",
    "\n",
    "df['euclidian_dist'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n",
    "                                       df['pickup_latitude'], df['dropoff_latitude'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance from the center \n",
    "- Compute a new Feature calculating distance of pickup location from the center\n",
    "- Scatter Plot *distance_from_center* regarding *distance* \n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute distance from NYC center\n",
    "nyc_center = (40.7141667, -74.0063889)\n",
    "df[\"nyc_lat\"], df[\"nyc_lng\"] = nyc_center[0], nyc_center[1]\n",
    "args =  dict(start_lat=\"nyc_lat\", start_lon=\"nyc_lng\",\n",
    "            end_lat=\"pickup_latitude\", end_lon=\"pickup_longitude\")\n",
    "\n",
    "df['distance_to_center'] = haversine_distance(df, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df.distance < 40) & (df.distance_to_center < 40)\n",
    "sns.scatterplot(x=\"distance_to_center\", y=\"distance\", data=df[idx].sample(10000), hue=\"fare-bin\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distance_to_center.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **Take time to step back conlude interesting pattern here ? What are these clustered with same distance to center?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to be fixed distance_to_center\n",
    "jfk_center = (40.6441666667, -73.7822222222)\n",
    "\n",
    "\n",
    "df[\"jfk_lat\"], df[\"jfk_lng\"] = jfk_center[0], jfk_center[1]\n",
    "args_pickup =  dict(start_lat=\"jfk_lat\", start_lon=\"jfk_lng\",\n",
    "            end_lat=\"pickup_latitude\", end_lon=\"pickup_longitude\")\n",
    "args_dropoff =  dict(start_lat=\"jfk_lat\", start_lon=\"jfk_lng\",\n",
    "            end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\")\n",
    "\n",
    "jfk = (-73.7822222222, 40.6441666667)\n",
    "df['pickup_distance_to_jfk'] = haversine_distance(df, **args_pickup)\n",
    "df['dropoff_distance_to_jfk'] = haversine_distance(df, **args_dropoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pickup_distance_to_jfk.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which direction  you heading to ?\n",
    "- Compute a new Feature calculating the direction your heading to\n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_direction(d_lon, d_lat):\n",
    "    result = np.zeros(len(d_lon))\n",
    "    l = np.sqrt(d_lon**2 + d_lat**2)\n",
    "    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n",
    "    idx = (d_lon<0) & (d_lat>0)\n",
    "    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n",
    "    idx = (d_lon<0) & (d_lat<0)\n",
    "    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_lon'] = df.pickup_longitude - df.dropoff_longitude\n",
    "df['delta_lat'] = df.pickup_latitude - df.dropoff_latitude\n",
    "df['direction'] = calculate_direction(df.delta_lon, df.delta_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "df.direction.hist(bins=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot direction vs average fare amount for fares inside manhattan\n",
    "def select_within_boundingbox(df, BB):\n",
    "    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "BB_manhattan = (-74.025, -73.925, 40.7, 40.8)\n",
    "idx_manhattan = select_within_boundingbox(df, BB_manhattan)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,6))\n",
    "direc = pd.cut(df[idx_manhattan]['direction'], np.linspace(-180, 180, 37))\n",
    "df[idx_manhattan].pivot_table('fare_amount', index=[direc], columns='year', aggfunc='mean').plot(ax=ax)\n",
    "plt.xlabel('direction (degrees)')\n",
    "plt.xticks(range(36), np.arange(-170, 190, 10))\n",
    "plt.ylabel('average fare amount $USD');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "l = list(corrs)\n",
    "l.remove(\"fare_amount\")\n",
    "corrs['fare_amount'][l].plot.bar(color = 'b');\n",
    "plt.title('Correlation with Fare Amount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
