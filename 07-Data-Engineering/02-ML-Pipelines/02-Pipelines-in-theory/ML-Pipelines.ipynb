{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of exercices, you will learn how a build robust a ML pipeline using [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "Also an important part of a ML pipeline is the pre-processing part. For this, you will learn how to master \n",
    "Sklearn encoders and tranformers as part of the [Preprocessing Sklearn module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Scaling with StandardScaler](#exo1)\n",
    "2. [Encoding Categorical Features](#exo2)\n",
    "3. [Dealing with missing data](#exo3)\n",
    "4. [Custom Transformers and Encoders](#exo4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scaling with StandardScaler <a id='exo1'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features by removing the mean and scaling to unit variance is a common pre-processing step we apply to help many machine learning algorithms behave more efficiently.\n",
    "\n",
    "[Sklearn StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) can do the scaling transformation for you.\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "As you know, there are 2 main methods for any encoder/transformer. \n",
    "- `fit` which computes the mean and std to be used for later scaling.\n",
    "- `tranform` which performs standardization by centering and scaling  \n",
    "\n",
    "ðŸŽ¯ The goal of this exercice is to understand the concepts behind `fit()` and `transfom()` by re-implementing the standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Given the numpy arrays `data` and `test_data`, write a simple custom implementation of standard scaler. To test it, fit the scaler with `data` and tranform `test_data` with it.\n",
    "- Compare your results with `StandardScaler`\n",
    "- Make the custom implementation using a python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 10], [2, -1], [0, 22], [3, 15]])\n",
    "test_data = np.array([[2, 1], [5, 1], [3, 55], [3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement fit() and transform() functions here\n",
    "\n",
    "def fit(X):\n",
    "    \"\"\"implement fit method\"\"\"\n",
    "    \n",
    "\n",
    "def transform(X, params):\n",
    "    \"\"\"implement transformation method\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = fit(data)\n",
    "transformed_test_data = transform(test_data, params)\n",
    "transformed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Sklearn StandardScaler and compare results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Implementation with a Class \n",
    "\n",
    "class Scaler(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"define parameters useful for fit and transform\"\"\"\n",
    "    \n",
    "    def fit(self, X):\n",
    "\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encoding Categorical Variables <a id='exo2'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often features are not given as continuous values but categorical. However, machine learning algorithms only accept numerical data as inputs. That is why we need to make sure categorical variables are encoded before passed in ML estimators.\n",
    "\n",
    "One encoder that is commonly used for categorical variables is [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "ðŸŽ¯ The goal of this exercice is to see a particular yet common case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Given `data` and `test_data`, use sklearn OneHotEncoder on these 2 sets, applying concepts from last exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([['France'], ['USA'], ['Italy'], ['Japan'], ['UK'], ['Germany'], ['USA'], ['Japan']])\n",
    "test_data = np.array([['China'], ['USA'], ['Italy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use Sklearn OneHotEncoder and compare results on test_data\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤”Why do you think you have this error ?   \n",
    "Solve this error using `handle_unknown` parameter from `OneHotEncoder()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve error here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with Missing Data <a id='exo3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning.\n",
    "\n",
    "For this, Sklearn has multiple ways to impute from missing data with the [Inpute module](https://scikit-learn.org/stable/modules/impute.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Re-implement the [`SimpleInputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) tranformer using `mean` strategy.\n",
    "- Test your implementation with `data` and `test_data`\n",
    "- Compare with transformed data using `Sklearn SimpleInputer`\n",
    "- Bonus: Implement for all 4 strategies (`mean`, `median`, `most_frequent` and `constant`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 3, 3], [2, np.nan, 6], [3, 9, 9]])\n",
    "test_data = np.array([[1, 1, 1], [1, np.nan, 1], [1, 1, 1]])\n",
    "data[np.isnan(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSimpleInputer(object):\n",
    "    \"\"\"Implement SimpleInputer \"\"\"\n",
    "\n",
    "    def __init__(self, strategy=\"mean\"):\n",
    "\n",
    "        \n",
    "    def fit(self, X, **kwargs):\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi = CustomSimpleInputer()\n",
    "csi.fit(data)\n",
    "csi.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use Sklearn Simple Inputer and compare transformed data using your custom implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom Transformers and Encoders <a id='exo4'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a large collection of transformers and encoders but you might need to implement you own encoder to fit the needs of your data and problem.\n",
    "\n",
    "For this, there are two very useful Sklearn classes:  \n",
    "ðŸ‘‰ [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) and [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) are base classes one can inherrit from to implement completely new custom encoders  \n",
    "ðŸ‘‰ Refer to this morning's slides to check how to implement custom transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the Taxi Fare Prediction Challenge data, using `BaseEstimator` and `TransformerMixin`, implement:\n",
    "\n",
    "- a transformer that computes haversine distance between pickup and dropoff location\n",
    "- a custom encoder that extract time features from `pickup_datetime`\n",
    "- Use these two new encoders to fit and transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv('s3://wagon-public-datasets/taxi-fare-train.csv', nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we Clean data using first exercice function\n",
    "def clean_df(df, test=False):\n",
    "    df = df.dropna(how='any', axis='rows')\n",
    "    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n",
    "    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "    if \"fare_amount\" in list(df):\n",
    "        df = df[df.fare_amount.between(0, 4000)]\n",
    "    df = df[df.passenger_count < 8]\n",
    "    df = df[df.passenger_count >= 0]\n",
    "    df = df[df[\"pickup_latitude\"].between(left = 40, right = 42 )]\n",
    "    df = df[df[\"pickup_longitude\"].between(left = -74.3, right = -72.9 )]\n",
    "    df = df[df[\"dropoff_latitude\"].between(left = 40, right = 42 )]\n",
    "    df = df[df[\"dropoff_longitude\"].between(left = -74, right = -72.9 )]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorized haversine function here\n",
    "def haversine_vectorized(df, \n",
    "    start_lat=\"start_lat\", \n",
    "    start_lon=\"start_lon\", \n",
    "    end_lat=\"end_lat\", \n",
    "    end_lon=\"end_lon\"):\n",
    "\n",
    "    \"\"\" \n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees).\n",
    "        Vectorized version of the haversine distance for pandas df\n",
    "        Computes distance in kms\n",
    "    \"\"\"\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n",
    "    dlon = lon_2_rad - lon_1_rad\n",
    "    dlat = lat_2_rad - lat_1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function here on df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Custom Transformer inheritting from BaseEstimator and TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"implement class here\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transfo = DistanceTransformer()\n",
    "hav_dist = custom_transfo.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hav_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(df.hav_dist.values) == list(hav_dist.distance.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd \n",
    "\n",
    "class TimeFeaturesEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_column, time_zone_name='America/New_York'):\n",
    "        self.time_column = time_column\n",
    "        self.time_zone_name = time_zone_name\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        X.index = pd.to_datetime(X[self.time_column])\n",
    "        X.index = X.index.tz_convert(self.time_zone_name)\n",
    "        X[\"dow\"] = X.index.weekday\n",
    "        X[\"hour\"] = X.index.hour\n",
    "        X[\"month\"] = X.index.month\n",
    "        X[\"year\"] = X.index.year\n",
    "        return X[[\"dow\", \"hour\", \"month\", \"year\"]].reset_index(drop=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TimeFeaturesEncoder(\"pickup_datetime\")\n",
    "tf.transform(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together as a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline is very useful concept. In Machine Learning, you often need to perform a sequence of different transformations (scaling, filling missing values, transforming, encoding) of raw dataset before applying a final estimator.\n",
    "\n",
    "A Pipeline gives you a simple interface for all these different steps of transformation and the resulting estimator. With that, it is easier to iterate and improve models because you can easily add, remove or re-order these different steps. Also, changing one or several parameters is very strightforward and does not require a lot code refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you will learn how to use 2 Sklearn modules:\n",
    "1. [ColumnTransformer](#exo11)\n",
    "2. [Pipeline](#exo12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Column Transformer <a id=\"exo11\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building your pipeline let's use a very useful Sklearn module called [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).\n",
    "\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.\n",
    "\n",
    "This module is very useful when your input data is a pandas dataframe as you can select columns from their names.\n",
    "\n",
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You are given a small dataset containing weights and heights for a few individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 180, 'weight': 82},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 72},\n",
    "        {'gender': 'Male', 'height': 175, 'weight': 75},\n",
    "        {'gender': 'Female', 'height': 175, 'weight': 60},\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 76},\n",
    "    ])\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 72},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 60}\n",
    "    ]\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ColumnTransformer`, build a single `encoder` that apply these transformations:\n",
    "- encode `gender` with OneHot\n",
    "- fill missing values for height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your encoder with ColumnTransformer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pipeline <a id=\"exo12\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use a Sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the weight/height dataset, build a pipeline to predict the weight of individuals in the test set.\n",
    "\n",
    "This pipeline should have:\n",
    "- a oneHotEncode for `gender`\n",
    "- fill missing values for height\n",
    "- a scaler for height\n",
    "- a simple estimator like a linear regression\n",
    "\n",
    "**Tip** You can also use [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) which is an alias of `Pipeline` to easily generate a pipeline without giving names to the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Pipeline here\n",
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(data, data.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
